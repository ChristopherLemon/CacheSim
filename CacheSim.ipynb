{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache Analysis Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import holoviews as hv\n",
    "import datashader as ds\n",
    "import pandas as pd\n",
    "import holoviews.operation.datashader as hd\n",
    "from holoviews.operation.datashader import aggregate, shade, datashade, dynspread, stack\n",
    "from datashader import transfer_functions as tf\n",
    "from holoviews.operation import decimate\n",
    "from IPython.core.display import display, HTML\n",
    "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "display(HTML(\"<style>.container { width:100% !important; height:100% important}</style>\"))\n",
    "hv.extension('bokeh')\n",
    "hv.notebook_extension('bokeh')\n",
    "decimate.max_samples=1000\n",
    "dynspread.max_px=20\n",
    "dynspread.threshold=0.5\n",
    "plot_width  = int(750)\n",
    "plot_height = int(plot_width//1.2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "offsets = {}\n",
    "data = {}\n",
    "memory_ops = {}\n",
    "n_lines = 0\n",
    "n_max_lines = sys.maxsize\n",
    "file_list = [\"MEMORY_DUMPS/memory.txt\"]\n",
    "regex_1 = r\"TID[0x]*([abcdef0-9]+).*INS(0x[abcdef0-9]+).*(R|W)0x([abcdef0-9]+)\"\n",
    "run_FIFO = False\n",
    "run_LRU = True\n",
    "start = timer()\n",
    "for file in file_list:\n",
    "    file_name = re.split('\\.',file)[0]\n",
    "    file_name = re.sub(\"/\",\"-\",file_name)\n",
    "    with open(file,'r') as file:\n",
    "        for line in file:\n",
    "            n_lines += 1\n",
    "            if n_lines > n_max_lines:\n",
    "                break\n",
    "            match = re.match(regex_1,line)\n",
    "            if match:\n",
    "                tid = int(match.group(1), 16)\n",
    "                ins = match.group(2)\n",
    "                memory_op = match.group(3)\n",
    "                data_name = file_name\n",
    "                thread_name = str(tid)\n",
    "                address = int(match.group(4), 16)\n",
    "                if data_name not in data:\n",
    "                    data[data_name] = {}\n",
    "                    offsets[data_name] = {}\n",
    "                    memory_ops[data_name] = {}\n",
    "                if thread_name not in data[data_name]:\n",
    "                    data[data_name][thread_name] = []\n",
    "                    memory_ops[data_name][thread_name] = []\n",
    "                    offsets[data_name][thread_name] = address\n",
    "                    print(\"data name: \", data_name, \", thread name: \", thread_name, \", memory op: \", memory_op, \", address: \", address)\n",
    "                memory_ops[data_name][thread_name].append(memory_op)\n",
    "                data[data_name][thread_name].append(address)\n",
    "end = timer()\n",
    "print('File read complete: ' + str(end - start))\n",
    "min_offset = {}\n",
    "for data_name in data:\n",
    "    if data_name not in min_offset:\n",
    "        min_offset[data_name] = sys.maxsize\n",
    "        for thread_name in data[data_name]:\n",
    "            offset = offsets[data_name][thread_name]\n",
    "            if offset < min_offset[data_name]:\n",
    "                min_offset[data_name] = offset\n",
    "max_len = {}\n",
    "n_data = {}\n",
    "thread_names = {}\n",
    "for data_name in data:\n",
    "    offset = min_offset[data_name]\n",
    "    n_data[data_name] = {}\n",
    "    max_len[data_name] = 0\n",
    "    thread_names[data_name] = []\n",
    "    for thread_name in data[data_name]:\n",
    "        thread_names[data_name].append(thread_name)\n",
    "        n_data[data_name][thread_name] = len(data[data_name][thread_name])\n",
    "        max_len[data_name] = max(max_len[data_name], n_data[data_name][thread_name])\n",
    "        for i in range(0, len(data[data_name][thread_name])):\n",
    "            address = data[data_name][thread_name][i]\n",
    "            data[data_name][thread_name][i] -= offset\n",
    "                \n",
    "for data_name in data:\n",
    "    new_data = []\n",
    "    mem_ops = []\n",
    "    for i in range(max_len[data_name]):\n",
    "        new_data.append({})\n",
    "        mem_ops.append({})\n",
    "        for thread_name in data[data_name]:\n",
    "            if i < n_data[data_name][thread_name]:\n",
    "                new_data[i][thread_name] = data[data_name][thread_name][i]\n",
    "                mem_ops[i][thread_name] = memory_ops[data_name][thread_name][i]\n",
    "    data[data_name] = new_data\n",
    "    memory_ops[data_name] = mem_ops\n",
    "end = timer()\n",
    "print(\"Data input complete: \" + str(n_lines) + \" lines. \" + str(end - start) + \" s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LRU Cache Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts RGB [width=plot_width, height=plot_height] {+axiswise}\n",
    "if run_LRU:\n",
    "    from collections import OrderedDict, defaultdict\n",
    "\n",
    "    start = timer()\n",
    "\n",
    "    cache_simulation_type = \"LRU\"\n",
    "\n",
    "    class CacheLine:\n",
    "\n",
    "        def __init__(self, id, previous_cache_line, next_cache_line, MEMORY_OP=\"\", TID=-1, modified=False):\n",
    "            self.id = id\n",
    "            self.previous_cache_line = previous_cache_line\n",
    "            self.next_cache_line = next_cache_line\n",
    "            self.last_TID = TID\n",
    "            self.last_MEMORY_OP = MEMORY_OP\n",
    "            self.MODIFIED = modified\n",
    "\n",
    "    cache_scale_factor = 1\n",
    "\n",
    "    L1_cache_size = 2**15 // cache_scale_factor\n",
    "    L1_cache_lines = OrderedDict()\n",
    "    L1_cache_misses = OrderedDict()\n",
    "    L2_cache_size = 2**18 // cache_scale_factor\n",
    "    L2_cache_lines = OrderedDict()\n",
    "    L2_cache_misses = OrderedDict()\n",
    "    L3_cache_size = 10 * (2**21) // cache_scale_factor\n",
    "    L3_cache_lines = OrderedDict()\n",
    "    L3_cache_misses = OrderedDict()\n",
    "    cache_line_size = 64\n",
    "    cache_misses = OrderedDict()\n",
    "    first_L1_cache_line = OrderedDict()\n",
    "    last_L1_cache_line = OrderedDict()\n",
    "    first_L2_cache_line = OrderedDict()\n",
    "    last_L2_cache_line = OrderedDict()\n",
    "    first_L3_cache_line = OrderedDict()\n",
    "    last_L3_cache_line = OrderedDict()\n",
    "    for data_name in data:\n",
    "        if data_name not in cache_misses:\n",
    "            cache_misses[data_name] = OrderedDict()\n",
    "        if data_name not in L1_cache_lines:\n",
    "            L1_cache_lines[data_name] = OrderedDict()\n",
    "            first_L1_cache_line[data_name] = OrderedDict()\n",
    "            last_L1_cache_line[data_name] = OrderedDict()\n",
    "        if data_name not in L2_cache_lines:\n",
    "            L2_cache_lines[data_name] = OrderedDict()\n",
    "            first_L2_cache_line[data_name] = OrderedDict()\n",
    "            last_L2_cache_line[data_name] = OrderedDict()\n",
    "        if data_name not in L3_cache_lines:\n",
    "            L3_cache_lines[data_name] = OrderedDict()\n",
    "            first_L3_cache_line[data_name] = None\n",
    "            last_L3_cache_line[data_name] = None\n",
    "        for thread_name in sorted(thread_names[data_name]):\n",
    "            if thread_name not in cache_misses[data_name]:\n",
    "                cache_misses[data_name][thread_name] = []\n",
    "            if thread_name not in L1_cache_lines[data_name]:\n",
    "                L1_cache_lines[data_name][thread_name] = OrderedDict()\n",
    "                first_L1_cache_line[data_name][thread_name] = None\n",
    "                last_L1_cache_line[data_name][thread_name] = None\n",
    "            if thread_name not in L2_cache_lines[data_name]:\n",
    "                L2_cache_lines[data_name][thread_name] = OrderedDict()\n",
    "                first_L2_cache_line[data_name][thread_name] = None\n",
    "                last_L2_cache_line[data_name][thread_name] = None\n",
    "        for i in range(max_len[data_name]):\n",
    "            for thread_name in data[data_name][i]:\n",
    "                address = data[data_name][i][thread_name]\n",
    "                memory_op = memory_ops[data_name][i][thread_name]\n",
    "                cache_line = address // cache_line_size\n",
    "                # Record Read/Write access\n",
    "                if memory_op == \"W\":\n",
    "                    for other_thread_name in data[data_name][i]:\n",
    "                        if thread_name == other_thread_name:\n",
    "                            break\n",
    "                        if cache_line in L1_cache_lines[data_name][other_thread_name]:\n",
    "                            L1_cache_lines[data_name][other_thread_name][cache_line].MODIFIED = True\n",
    "                        if cache_line in L2_cache_lines[data_name][other_thread_name]:\n",
    "                            L2_cache_lines[data_name][other_thread_name][cache_line].MODIFIED = True\n",
    "            for thread_name in data[data_name][i]:\n",
    "                address = data[data_name][i][thread_name]\n",
    "                memory_op = memory_ops[data_name][i][thread_name]\n",
    "                cache_line = address // cache_line_size\n",
    "                cache_misses[data_name][thread_name].append(1)    \n",
    "                modified = False\n",
    "                shared = False\n",
    "                # L1 Cache Miss\n",
    "                if cache_line not in L1_cache_lines[data_name][thread_name]:\n",
    "                    # L1 Cache full - remove least recently used line\n",
    "                    if cache_line_size * len(L1_cache_lines[data_name][thread_name]) == L1_cache_size:\n",
    "                        dead_cache_line = last_L1_cache_line[data_name][thread_name]\n",
    "                        last_L1_cache_line[data_name][thread_name] = dead_cache_line.next_cache_line\n",
    "                        last_L1_cache_line[data_name][thread_name].previous_cache_line = None\n",
    "                        del L1_cache_lines[data_name][thread_name][dead_cache_line.id]\n",
    "                        L1_cache_lines[data_name][thread_name][last_L1_cache_line[data_name][thread_name].id] = last_L1_cache_line[data_name][thread_name]\n",
    "                    next_L1_cache_line = None\n",
    "                    # Create new L1 Cache Line\n",
    "                    new_cache_line = CacheLine(cache_line, first_L1_cache_line[data_name][thread_name], next_L1_cache_line, thread_name)\n",
    "                    L1_cache_lines[data_name][thread_name][cache_line] = new_cache_line\n",
    "                    previous_cache_line = first_L1_cache_line[data_name][thread_name]\n",
    "                    if previous_cache_line:\n",
    "                        previous_cache_line.next_cache_line = new_cache_line\n",
    "                        first_L1_cache_line[data_name][thread_name] = new_cache_line\n",
    "                    else:\n",
    "                        first_L1_cache_line[data_name][thread_name] = new_cache_line\n",
    "                        last_L1_cache_line[data_name][thread_name] = new_cache_line\n",
    "                    cache_misses[data_name][thread_name][-1] = 2\n",
    "                # L1 Cache Hit\n",
    "                else:\n",
    "                    # Extract this cache line from the list, patch up the hole in the list, and add the cache line to the end of the list\n",
    "                    this_cache_line = L1_cache_lines[data_name][thread_name][cache_line]\n",
    "                    first_cache_line = first_L1_cache_line[data_name][thread_name]\n",
    "                    if this_cache_line != first_cache_line:\n",
    "                        if this_cache_line.next_cache_line:\n",
    "                            if this_cache_line.previous_cache_line:\n",
    "                                this_cache_line.previous_cache_line.next_cache_line = this_cache_line.next_cache_line\n",
    "                                this_cache_line.next_cache_line.previous_cache_line = this_cache_line.previous_cache_line\n",
    "                            else:\n",
    "                                last_cache_line = this_cache_line.next_cache_line\n",
    "                                last_cache_line.previous_cache_line = None\n",
    "                                last_L1_cache_line[data_name][thread_name] = last_cache_line\n",
    "                                first_cache_line.next_cache_line = this_cache_line\n",
    "                                this_cache_line.previous_cache_line = first_cache_line\n",
    "                                first_L1_cache_line[data_name][thread_name] = this_cache_line\n",
    "                # L2 Cache Miss\n",
    "                if cache_line not in L2_cache_lines[data_name][thread_name]:\n",
    "                    if cache_line_size * len(L2_cache_lines[data_name][thread_name]) == L2_cache_size:\n",
    "                        dead_cache_line = last_L2_cache_line[data_name][thread_name]\n",
    "                        last_L2_cache_line[data_name][thread_name] = dead_cache_line.next_cache_line\n",
    "                        last_L2_cache_line[data_name][thread_name].previous_cache_line = None\n",
    "                        del L2_cache_lines[data_name][thread_name][dead_cache_line.id]\n",
    "                        L2_cache_lines[data_name][thread_name][last_L2_cache_line[data_name][thread_name].id] = last_L2_cache_line[data_name][thread_name]\n",
    "                    next_L2_cache_line = None\n",
    "                    new_cache_line = CacheLine(cache_line, first_L2_cache_line[data_name][thread_name], next_L2_cache_line, thread_name)\n",
    "                    L2_cache_lines[data_name][thread_name][cache_line] = new_cache_line\n",
    "                    previous_cache_line = first_L2_cache_line[data_name][thread_name]\n",
    "                    if previous_cache_line:\n",
    "                        previous_cache_line.next_cache_line = new_cache_line\n",
    "                        first_L2_cache_line[data_name][thread_name] = new_cache_line\n",
    "                    else:\n",
    "                        first_L2_cache_line[data_name][thread_name] = new_cache_line\n",
    "                        last_L2_cache_line[data_name][thread_name] = new_cache_line\n",
    "                    cache_misses[data_name][thread_name][-1] = 3\n",
    "                # L2 Cache Hit\n",
    "                else:\n",
    "                    this_cache_line = L2_cache_lines[data_name][thread_name][cache_line]\n",
    "                    first_cache_line = first_L2_cache_line[data_name][thread_name]\n",
    "                    if this_cache_line != first_cache_line:\n",
    "                        if this_cache_line.next_cache_line:\n",
    "                            if this_cache_line.previous_cache_line:\n",
    "                                this_cache_line.previous_cache_line.next_cache_line = this_cache_line.next_cache_line\n",
    "                                this_cache_line.next_cache_line.previous_cache_line = this_cache_line.previous_cache_line\n",
    "                            else:\n",
    "                                last_cache_line = this_cache_line.next_cache_line\n",
    "                                last_cache_line.previous_cache_line = None\n",
    "                                last_L2_cache_line[data_name][thread_name] = last_cache_line\n",
    "                                first_cache_line.next_cache_line = this_cache_line\n",
    "                                this_cache_line.previous_cache_line = first_cache_line\n",
    "                                first_L2_cache_line[data_name][thread_name] = this_cache_line\n",
    "                # L3 Cache Miss\n",
    "                if cache_line not in L3_cache_lines[data_name]:\n",
    "                    if cache_line_size * len(L3_cache_lines[data_name]) == L3_cache_size:\n",
    "                        dead_cache_line = last_L3_cache_line[data_name]\n",
    "                        last_L3_cache_line[data_name] = dead_cache_line.next_cache_line\n",
    "                        last_L3_cache_line[data_name].previous_cache_line = None\n",
    "                        del L3_cache_lines[data_name][dead_cache_line.id]\n",
    "                        L3_cache_lines[data_name][last_L3_cache_line[data_name].id] = last_L3_cache_line[data_name]\n",
    "                    next_L3_cache_line = None\n",
    "                    new_cache_line = CacheLine(cache_line, first_L3_cache_line[data_name], next_L3_cache_line, thread_name)\n",
    "                    L3_cache_lines[data_name][cache_line] = new_cache_line\n",
    "                    previous_cache_line = first_L3_cache_line[data_name]\n",
    "                    if previous_cache_line:\n",
    "                        previous_cache_line.next_cache_line = new_cache_line\n",
    "                        first_L3_cache_line[data_name] = new_cache_line\n",
    "                    else:\n",
    "                        first_L3_cache_line[data_name] = new_cache_line\n",
    "                        last_L3_cache_line[data_name] = new_cache_line\n",
    "                    cache_misses[data_name][thread_name][-1] = 4\n",
    "                # L3 Cache Hit\n",
    "                else:\n",
    "                    if L3_cache_lines[data_name][cache_line].last_TID != thread_name:\n",
    "                        if cache_misses[data_name][thread_name][-1] == 3:\n",
    "                            shared = True\n",
    "                    L3_cache_lines[data_name][cache_line].last_TID = thread_name\n",
    "                    this_cache_line = L3_cache_lines[data_name][cache_line]\n",
    "                    first_cache_line = first_L3_cache_line[data_name]\n",
    "                    if this_cache_line != first_cache_line:\n",
    "                        if this_cache_line.next_cache_line:\n",
    "                            if this_cache_line.previous_cache_line:\n",
    "                                this_cache_line.previous_cache_line.next_cache_line = this_cache_line.next_cache_line\n",
    "                                this_cache_line.next_cache_line.previous_cache_line = this_cache_line.previous_cache_line\n",
    "                            else:\n",
    "                                last_cache_line = this_cache_line.next_cache_line\n",
    "                                last_cache_line.previous_cache_line = None\n",
    "                                last_L3_cache_line[data_name] = last_cache_line\n",
    "                                first_cache_line.next_cache_line = this_cache_line\n",
    "                                this_cache_line.previous_cache_line = first_cache_line\n",
    "                                first_L3_cache_line[data_name] = this_cache_line\n",
    "                # L1 cache line was modified by another thread\n",
    "                if L1_cache_lines[data_name][thread_name][cache_line].MODIFIED:\n",
    "                    modified = True\n",
    "                    L1_cache_lines[data_name][thread_name][cache_line].MODIFIED = False\n",
    "                # L2 cache line was modified by another thread\n",
    "                if L2_cache_lines[data_name][thread_name][cache_line].MODIFIED:\n",
    "                    modified = True\n",
    "                    L2_cache_lines[data_name][thread_name][cache_line].MODIFIED = False\n",
    "                # Mark cache line as modified\n",
    "                if modified:\n",
    "                    cache_misses[data_name][thread_name][-1] = 5\n",
    "                # L3 Cache line was previously accessed by another thread. Mark as shared cache line\n",
    "                if shared:\n",
    "                    cache_misses[data_name][thread_name][-1] += 10\n",
    "                # Mark as writ operation\n",
    "                if memory_op == \"W\":\n",
    "                    cache_misses[data_name][thread_name][-1] += 100\n",
    "                        \n",
    "    summary_data = OrderedDict()\n",
    "    for data_name in cache_misses:\n",
    "        summary_data[data_name] = OrderedDict()\n",
    "        summary_data[data_name][\"All\"] = defaultdict(int)\n",
    "        for thread_name in cache_misses[data_name]:\n",
    "            summary_data[data_name][thread_name] = defaultdict(int)\n",
    "            summary_data[data_name][thread_name][\"Cache_References\"] = len(cache_misses[data_name][thread_name])\n",
    "            for i in range(len(cache_misses[data_name][thread_name])):\n",
    "                cache_level = cache_misses[data_name][thread_name][i]\n",
    "                l = cache_level\n",
    "                if l // 100 == 0:\n",
    "                    summary_data[data_name][thread_name][\"Reads\"] += 1\n",
    "                else: \n",
    "                    summary_data[data_name][thread_name][\"Writes\"] += 1\n",
    "                l = (l % 100)\n",
    "                if l // 10 == 1:\n",
    "                    summary_data[data_name][thread_name][\"L3_Hits_Shared\"] += 1\n",
    "                l = (l % 10)\n",
    "                if l == 1:\n",
    "                    summary_data[data_name][thread_name][\"L1_Cache_Hits\"] += 1\n",
    "                elif l == 2:\n",
    "                    summary_data[data_name][thread_name][\"L2_Cache_Hits\"] += 1\n",
    "                elif l == 3:\n",
    "                    summary_data[data_name][thread_name][\"L3_Cache_Hits\"] += 1\n",
    "                elif l == 4:\n",
    "                    summary_data[data_name][thread_name][\"L3_Cache_Misses\"] += 1\n",
    "                elif l == 5:\n",
    "                    summary_data[data_name][thread_name][\"Cache_Hits_Modified\"] += 1\n",
    "        for thread_name in summary_data[data_name]:\n",
    "            summary_data[data_name][\"All\"][\"Cache_References\"] += summary_data[data_name][thread_name][\"Cache_References\"]\n",
    "            summary_data[data_name][\"All\"][\"Reads\"] += summary_data[data_name][thread_name][\"Reads\"]\n",
    "            summary_data[data_name][\"All\"][\"Writes\"] += summary_data[data_name][thread_name][\"Writes\"]\n",
    "            summary_data[data_name][\"All\"][\"L1_Cache_Hits\"] += summary_data[data_name][thread_name][\"L1_Cache_Hits\"]\n",
    "            summary_data[data_name][\"All\"][\"L2_Cache_Hits\"] += summary_data[data_name][thread_name][\"L2_Cache_Hits\"]\n",
    "            summary_data[data_name][\"All\"][\"L3_Cache_Hits\"] += summary_data[data_name][thread_name][\"L3_Cache_Hits\"]\n",
    "            summary_data[data_name][\"All\"][\"L3_Cache_Misses\"] += summary_data[data_name][thread_name][\"L3_Cache_Misses\"]\n",
    "            summary_data[data_name][\"All\"][\"Cache_Hits_Modified\"] += summary_data[data_name][thread_name][\"Cache_Hits_Modified\"]\n",
    "            summary_data[data_name][\"All\"][\"L3_Hits_Shared\"] += summary_data[data_name][thread_name][\"L3_Hits_Shared\"]\n",
    "    for data_name in summary_data:\n",
    "        for thread_name in sorted(summary_data[data_name].keys()):\n",
    "            for label in sorted(summary_data[data_name][thread_name].keys()):\n",
    "                val = summary_data[data_name][thread_name][label]\n",
    "                total = summary_data[data_name][thread_name][\"Cache_References\"]\n",
    "                pc = '{0:.2F}'.format(100.0 * float(val) / float(total))\n",
    "                print(data_name + \"---\" + thread_name + \"---\" + label + \": \" + str(val) + \" (\" + pc + \"%)\")\n",
    "\n",
    "    end = timer()\n",
    "\n",
    "    print('LRU cache simulation complete: ' + str(end - start) + \" s\")\n",
    "else:\n",
    "    print('LRU cache simulation skipped')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Cache Sim Totals: All Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Bars [width=plot_width, height=plot_height] {+axiswise}\n",
    "print(\"Cache simulation: \" + cache_simulation_type)\n",
    "start = timer()\n",
    "d_s = {'data_name':[], 'thread_id': [], 'property': [], 'value': []}\n",
    "for data_name in sorted(summary_data.keys()):\n",
    "    for thread_name in sorted(summary_data[data_name].keys()):\n",
    "        for property_name in sorted(summary_data[data_name][thread_name].keys()):\n",
    "            d_s['data_name'].append(data_name)\n",
    "            d_s['thread_id'].append(thread_name)\n",
    "            d_s['property'].append(property_name)\n",
    "            d_s['value'].append(summary_data[data_name][thread_name][property_name])\n",
    "df_s = pd.DataFrame(data=d_s)\n",
    "plots = {}\n",
    "for data_name in sorted(summary_data.keys()):\n",
    "    for property_name in sorted(summary_data[data_name][\"All\"].keys()):\n",
    "        plots[(data_name, property_name)] = hv.Bars(df_s.loc[(df_s['data_name'] == data_name) & (df_s['property'] == property_name)], kdims=['thread_id'], vdims=['value'])\n",
    "holomap = hv.HoloMap(plots, kdims=['data_name','property_name'])\n",
    "end = timer()\n",
    "print(\"Time: \" + str(end - start) + \" s\")\n",
    "holomap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Cache Sim Totals: All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Bars [width=plot_width, height=plot_height] {+axiswise}\n",
    "print(\"Cache simulation: \" + cache_simulation_type)\n",
    "start = timer()\n",
    "d_s = {'data_name':[], 'thread_id': [], 'property': [], 'value': []}\n",
    "for data_name in sorted(summary_data.keys()):\n",
    "    for thread_name in sorted(summary_data[data_name].keys()):\n",
    "        for property_name in sorted(summary_data[data_name][thread_name].keys()):\n",
    "            d_s['data_name'].append(data_name)\n",
    "            d_s['thread_id'].append(thread_name)\n",
    "            d_s['property'].append(property_name)\n",
    "            d_s['value'].append(summary_data[data_name][thread_name][property_name])\n",
    "df_s = pd.DataFrame(data=d_s)\n",
    "plots = {}\n",
    "all_threads = []\n",
    "for data_name in summary_data:\n",
    "    for thread_name in summary_data[data_name]:\n",
    "        if thread_name not in all_threads:\n",
    "            all_threads.append(thread_name)\n",
    "properties = [\"Cache_References\", \"Reads\", \"Writes\", \"L1_Cache_Hits\", \"L2_Cache_Hits\", \"L3_Cache_Hits\", \"L3_Cache_Misses\", \"Cache_Hits_Modified\", \"L3_Hits_Shared\"]\n",
    "for thread_name in sorted(all_threads):\n",
    "    for property_name in properties:\n",
    "        plots[(thread_name, property_name)] = hv.Bars(df_s.loc[(df_s['thread_id'] == thread_name) & (df_s['property'] == property_name)], kdims=['data_name'], vdims=['value'])\n",
    "holomap = hv.HoloMap(plots, kdims=['thread_id','property_name'])\n",
    "end = timer()\n",
    "print(\"Time: \" + str(end - start) + \" s\")\n",
    "holomap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Cache Sim: Per Cache Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%opts RGB [width=plot_width, height=plot_height] {+axiswise}\n",
    "print(\"Cache simulation: \" + cache_simulation_type)\n",
    "start = timer()\n",
    "cache_levels = []\n",
    "row_count = {}\n",
    "data_row = {}\n",
    "thread_id_to_name = {}\n",
    "thread_name_to_id = {}\n",
    "n_threads = 0\n",
    "data_frames = OrderedDict()\n",
    "minx = 0\n",
    "miny = 0\n",
    "maxx = 0\n",
    "maxy = 0\n",
    "elements_per_row = 10\n",
    "cache_levels = [\"Cache_References\", \"Reads\", \"Writes\", \"L3_Hits_Shared\", \"L1_Cache_Hits\", \"L2_Cache_Hits\", \"L3_Cache_Hits\", \"L3_Cache_Misses\", \"Cache_Hits_Modified\"]\n",
    "for data_name in data:\n",
    "    d = OrderedDict()\n",
    "    for cache_level in cache_levels:\n",
    "        d[cache_level] = {'thread_id': [], 'address': [], 'data_row': []}\n",
    "    if data_name not in data_row:\n",
    "        data_row[data_name] = {}\n",
    "        row_count[data_name] = {}\n",
    "    for thread_name in sorted(thread_names[data_name]):\n",
    "        if thread_name not in thread_name_to_id:\n",
    "            thread_id_to_name[n_threads] = thread_name\n",
    "            thread_name_to_id[thread_name] = n_threads\n",
    "            n_threads += 1\n",
    "        if thread_name not in data_row[data_name]:\n",
    "            data_row[data_name][thread_name] = 0\n",
    "            row_count[data_name][thread_name] = 0\n",
    "        for cache_level in cache_levels: # Append dummy data to prevent issues with empty data intersections\n",
    "            d[cache_level]['thread_id'].append(thread_name_to_id[thread_name])\n",
    "            d[cache_level]['address'].append(0)\n",
    "            d[cache_level]['data_row'].append(-1)\n",
    "    for i in range(max_len[data_name]):\n",
    "        for thread_name in data[data_name][i]:\n",
    "            address = data[data_name][i][thread_name]\n",
    "            cache_level = cache_misses[data_name][thread_name][i]\n",
    "            l = cache_level\n",
    "            d[\"Cache_References\"]['thread_id'].append(thread_name_to_id[thread_name])\n",
    "            d[\"Cache_References\"]['address'].append(address)\n",
    "            d[\"Cache_References\"]['data_row'].append(data_row[data_name][thread_name])\n",
    "            if l // 100 == 0:\n",
    "                d[\"Reads\"]['thread_id'].append(thread_name_to_id[thread_name])\n",
    "                d[\"Reads\"]['address'].append(address)\n",
    "                d[\"Reads\"]['data_row'].append(data_row[data_name][thread_name])\n",
    "            else: \n",
    "                d[\"Writes\"]['thread_id'].append(thread_name_to_id[thread_name])\n",
    "                d[\"Writes\"]['address'].append(address)\n",
    "                d[\"Writes\"]['data_row'].append(data_row[data_name][thread_name])\n",
    "            l = (l % 100)\n",
    "            if l // 10 == 1:\n",
    "                d[\"L3_Hits_Shared\"]['thread_id'].append(thread_name_to_id[thread_name])\n",
    "                d[\"L3_Hits_Shared\"]['address'].append(address)\n",
    "                d[\"L3_Hits_Shared\"]['data_row'].append(data_row[data_name][thread_name])\n",
    "            l = (l % 10)\n",
    "            if l == 1:\n",
    "                d[\"L1_Cache_Hits\"]['thread_id'].append(thread_name_to_id[thread_name])\n",
    "                d[\"L1_Cache_Hits\"]['address'].append(address)\n",
    "                d[\"L1_Cache_Hits\"]['data_row'].append(data_row[data_name][thread_name])\n",
    "            elif l == 2:\n",
    "                d[\"L2_Cache_Hits\"]['thread_id'].append(thread_name_to_id[thread_name])\n",
    "                d[\"L2_Cache_Hits\"]['address'].append(address)\n",
    "                d[\"L2_Cache_Hits\"]['data_row'].append(data_row[data_name][thread_name])\n",
    "            elif l == 3:\n",
    "                d[\"L3_Cache_Hits\"]['thread_id'].append(thread_name_to_id[thread_name])\n",
    "                d[\"L3_Cache_Hits\"]['address'].append(address)\n",
    "                d[\"L3_Cache_Hits\"]['data_row'].append(data_row[data_name][thread_name])\n",
    "            elif l == 4:\n",
    "                d[\"L3_Cache_Misses\"]['thread_id'].append(thread_name_to_id[thread_name])\n",
    "                d[\"L3_Cache_Misses\"]['address'].append(address)\n",
    "                d[\"L3_Cache_Misses\"]['data_row'].append(data_row[data_name][thread_name])\n",
    "            elif l == 5:\n",
    "                d[\"Cache_Hits_Modified\"]['thread_id'].append(thread_name_to_id[thread_name])\n",
    "                d[\"Cache_Hits_Modified\"]['address'].append(address)\n",
    "                d[\"Cache_Hits_Modified\"]['data_row'].append(data_row[data_name][thread_name])\n",
    "            maxx = max(maxx, address)\n",
    "            minx = min(minx, address)\n",
    "            row_count[data_name][thread_name] += 1\n",
    "            if row_count[data_name][thread_name] > elements_per_row:\n",
    "                data_row[data_name][thread_name] += 1\n",
    "                row_count[data_name][thread_name] = 0\n",
    "            maxy = max(maxy, data_row[data_name][thread_name])\n",
    "    data_frames[data_name] = OrderedDict()\n",
    "    for cache_level in cache_levels:\n",
    "        data_frames[data_name][cache_level] = pd.DataFrame(data=d[cache_level])\n",
    "            \n",
    "end = timer()\n",
    "print(\"Time: \" + str(end - start) + \" s\")\n",
    "thread_keys = [\"Thread \" + str(i) for i in range(n_threads)]\n",
    "color_key = OrderedDict((k, c) for k, c in zip(thread_keys, Sets1to3[0:len(thread_keys)]))\n",
    "colors = hv.NdOverlay(OrderedDict((c, hv.Points([0,0], label=c).opts(style=dict(color=color_key[c]))) for c in thread_keys))\n",
    "scatter_dict = OrderedDict()\n",
    "opts = dict(x_range=(minx, maxx), y_range=(miny, maxy))\n",
    "for data_name in data:\n",
    "    for cache_level in cache_levels:\n",
    "        df = data_frames[data_name][cache_level]\n",
    "        plot = OrderedDict((k, hv.Scatter(df.loc[(df['thread_id'] == k)], kdims=['address', 'data_row'])) for k in range(n_threads))\n",
    "        plot_data = hv.NdOverlay(plot, kdims='thread_keys')\n",
    "        scatter_dict[(data_name, cache_level)] = plot_data\n",
    "hmap = dynspread(datashade(hv.HoloMap(scatter_dict, kdims=['data_name', 'cache_level']), aggregator=ds.count_cat('thread_keys'), **opts), threshold=0.75, how='over') * colors\n",
    "end = timer()\n",
    "print(\"Time: \" + str(end - start) + \" s\")\n",
    "hmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Cache Sim: All Cache Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts RGB [width=plot_width, height=plot_height] {+axiswise}\n",
    "print(\"Cache simulation: \" + cache_simulation_type)\n",
    "start = timer()\n",
    "row_count = {}\n",
    "thread_id_to_name = {}\n",
    "thread_name_to_id = {}\n",
    "data_row = {}\n",
    "n_threads = 0\n",
    "data_frames = OrderedDict()\n",
    "minx = 0\n",
    "miny = 0\n",
    "maxx = 0\n",
    "maxy = 0\n",
    "elements_per_row = 10\n",
    "for data_name in data:\n",
    "    d = OrderedDict()\n",
    "    for thread_name in thread_names[data_name] + [\"All\"]:\n",
    "        d[thread_name] = {'address': [], 'data_row': [], 'cache_level': []}\n",
    "    if data_name not in data_row:\n",
    "        data_row[data_name] = {}\n",
    "        row_count[data_name] = {}\n",
    "    for thread_name in thread_names[data_name] + [\"All\"]:\n",
    "        n_threads += 1\n",
    "        if thread_name not in data_row[data_name]:\n",
    "            data_row[data_name][thread_name] = 0\n",
    "            row_count[data_name][thread_name] = 0\n",
    "        for hit in [1, 2, 3, 4, 5, 6]: # Append dummy data to prevent issues with empty data intersections\n",
    "            d[thread_name]['cache_level'].append(hit)\n",
    "            d[thread_name]['address'].append(0)\n",
    "            d[thread_name]['data_row'].append(-1)\n",
    "    for i in range(max_len[data_name]):\n",
    "        for thread_name in data[data_name][i]:\n",
    "            address = data[data_name][i][thread_name]\n",
    "            maxx = max(maxx, address)\n",
    "            cache_level = cache_misses[data_name][thread_name][i]\n",
    "            l = cache_level\n",
    "            l = (l % 100)\n",
    "            m = (l % 10)\n",
    "            if m == 5:\n",
    "                hit = 6\n",
    "            elif m == 1:\n",
    "                hit = 1\n",
    "            elif m == 2:\n",
    "                hit = 2\n",
    "            elif l // 10 == 1:\n",
    "                hit = 3\n",
    "            elif m == 3:\n",
    "                hit = 4\n",
    "            elif m == 4:\n",
    "                hit = 5\n",
    "            else:\n",
    "                print(\"No hit: \", str(cache_level))\n",
    "                continue\n",
    "            d[thread_name]['address'].append(address)\n",
    "            d[thread_name]['data_row'].append(data_row[data_name][thread_name])\n",
    "            d[thread_name]['cache_level'].append(hit)\n",
    "            d[\"All\"]['address'].append(address)\n",
    "            d[\"All\"]['data_row'].append(data_row[data_name][thread_name])  \n",
    "            d[\"All\"]['cache_level'].append(hit)\n",
    "            minx = min(minx, address)\n",
    "            row_count[data_name][thread_name] += 1\n",
    "            if row_count[data_name][thread_name] > elements_per_row:\n",
    "                data_row[data_name][thread_name] += 1\n",
    "                row_count[data_name][thread_name] = 0\n",
    "            maxy = max(maxy, data_row[data_name][thread_name])\n",
    "    data_frames[data_name] = OrderedDict()\n",
    "    for thread_name in thread_names[data_name] + [\"All\"]:\n",
    "        data_frames[data_name][thread_name] = pd.DataFrame(data=d[thread_name])\n",
    "end = timer()\n",
    "print(\"Time: \" + str(end - start) + \" s\")\n",
    "cache_levels = [1, 2, 3, 4, 5, 6]\n",
    "cache_level_keys = [\"L1 Hit\", \"L2 Hit\", \"L3 Hit Shared\", \"L3 Hit\", \"L3 Miss\", \"Modified\"]\n",
    "color_key = OrderedDict((k, c) for k, c in zip(cache_level_keys, Sets1to3[0:len(cache_level_keys)]))\n",
    "colors = hv.NdOverlay(OrderedDict((c, hv.Points([0,0], label=c).opts(style=dict(color=color_key[c]))) for c in cache_level_keys))\n",
    "scatter_dict = OrderedDict()\n",
    "opts = dict(x_range=(minx, maxx), y_range=(miny, maxy))\n",
    "threads = [str(th) for th in sorted(thread_names[data_name], key=lambda x: int(x))] + [\"All\"]\n",
    "for data_name in data:\n",
    "    for thread_name in threads:\n",
    "        df = data_frames[data_name][thread_name]\n",
    "        plot = OrderedDict((cache_level, hv.Scatter(df.loc[(df['cache_level'] == cache_level)], kdims=['address', 'data_row'])) for cache_level in cache_levels)\n",
    "        plot_data = hv.NdOverlay(plot, kdims='cache_level_keys')\n",
    "        scatter_dict[(data_name, thread_name)] = plot_data\n",
    "hmap = dynspread(datashade(hv.HoloMap(scatter_dict, kdims=['data_name', 'thread_name']), aggregator=ds.count_cat('cache_level_keys'), **opts), threshold=0.75, how='over') * colors\n",
    "end = timer()\n",
    "print(\"Time: \" + str(end - start) + \" s\")\n",
    "hmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Access by Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%opts RGB [width=plot_width, height=plot_height] {+axiswise}\n",
    "print(\"Cache simulation: \" + cache_simulation_type)\n",
    "start = timer()\n",
    "last_address = {}\n",
    "thread_id_to_name = {}\n",
    "thread_name_to_id = {}\n",
    "last_access = {}\n",
    "n_threads = 0\n",
    "data_frames = OrderedDict()\n",
    "minx = 0\n",
    "miny = 0\n",
    "maxx = 0\n",
    "maxy = 0\n",
    "for data_name in data:\n",
    "    d = OrderedDict()\n",
    "    for thread_name in thread_names[data_name] + [\"All\"]:\n",
    "        d[thread_name] = {'address': [], 'last_access': [], 'cache_level': []}\n",
    "    if data_name not in last_access:\n",
    "        last_access[data_name] = {}\n",
    "    for thread_name in thread_names[data_name] + [\"All\"]:\n",
    "        n_threads += 1\n",
    "        if thread_name not in last_access[data_name]:\n",
    "            last_access[data_name][thread_name] = {}\n",
    "    for i in range(max_len[data_name]):\n",
    "        for thread_name in data[data_name][i]:\n",
    "            address = data[data_name][i][thread_name]\n",
    "            cache_level = cache_misses[data_name][thread_name][i]\n",
    "            if address in last_access[data_name][thread_name]:\n",
    "                dl = i - last_access[data_name][thread_name][address]\n",
    "            else:\n",
    "                dl = 0\n",
    "            last_access[data_name][thread_name][address] = i\n",
    "            maxx = max(maxx, address)\n",
    "            minx = min(minx, address)\n",
    "            l = cache_level\n",
    "            l = (l % 100)\n",
    "            m = (l % 10)\n",
    "            if m == 5:\n",
    "                hit = 6\n",
    "            elif m == 1:\n",
    "                hit = 1\n",
    "            elif m == 2:\n",
    "                hit = 2\n",
    "            elif l // 10 == 1:\n",
    "                hit = 3\n",
    "            elif m == 3:\n",
    "                hit = 4\n",
    "            elif m == 4:\n",
    "                hit = 5\n",
    "            else:\n",
    "                print(\"No hit: \", str(cache_level))\n",
    "                continue\n",
    "            d[thread_name]['address'].append(address)\n",
    "            d[thread_name]['last_access'].append(dl)\n",
    "            d[thread_name]['cache_level'].append(hit)\n",
    "            d[\"All\"]['address'].append(address)\n",
    "            d[\"All\"]['last_access'].append(dl)  \n",
    "            d[\"All\"]['cache_level'].append(hit)\n",
    "            maxy = max(maxy, dl)\n",
    "    data_frames[data_name] = OrderedDict()\n",
    "    for thread_name in thread_names[data_name] + [\"All\"]:\n",
    "        data_frames[data_name][thread_name] = pd.DataFrame(data=d[thread_name])\n",
    "end = timer()\n",
    "print(\"Time: \" + str(end - start) + \" s\")\n",
    "cache_levels = [1, 2, 3, 4, 5, 6]\n",
    "cache_level_keys = [\"L1 Hit\", \"L2 Hit\", \"L3 Hit Shared\", \"L3 Hit\", \"L3 Miss\", \"Modified\"]\n",
    "color_key = OrderedDict((k, c) for k, c in zip(cache_level_keys, Sets1to3[0:len(cache_level_keys)]))\n",
    "colors = hv.NdOverlay(OrderedDict((c, hv.Points([0,0], label=c).opts(style=dict(color=color_key[c]))) for c in cache_level_keys))\n",
    "scatter_dict = OrderedDict()\n",
    "opts = dict(x_range=(minx, maxx), y_range=(miny, maxy))\n",
    "threads = [str(th) for th in sorted(thread_names[data_name], key=lambda x: int(x))] + [\"All\"]\n",
    "for data_name in data:\n",
    "    for thread_name in threads:\n",
    "        df = data_frames[data_name][thread_name]\n",
    "        plot = OrderedDict((cache_level, hv.Scatter(df.loc[(df['cache_level'] == cache_level)], kdims=['address', 'last_access'])) for cache_level in cache_levels)\n",
    "        plot_data = hv.NdOverlay(plot, kdims='cache_level_keys')\n",
    "        scatter_dict[(data_name, thread_name)] = plot_data\n",
    "hmap = dynspread(datashade(hv.HoloMap(scatter_dict, kdims=['data_name', 'thread_name']), aggregator=ds.count_cat('cache_level_keys'), **opts), threshold=0.75, how='over') * colors\n",
    "end = timer()\n",
    "print(\"Time: \" + str(end - start) + \" s\")\n",
    "hmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Access Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts RGB [width=plot_width, height=plot_height] {+axiswise}\n",
    "print(\"Cache simulation: \" + cache_simulation_type)\n",
    "start = timer()\n",
    "last_address = {}\n",
    "data_row = {}\n",
    "thread_id_to_name = {}\n",
    "thread_name_to_id = {}\n",
    "data_count = {}\n",
    "n_threads = 0\n",
    "data_frames = OrderedDict()\n",
    "minx = 0\n",
    "miny = 0\n",
    "maxx = 0\n",
    "maxy = 0\n",
    "for data_name in data:\n",
    "    if data_name not in data_count:\n",
    "        data_count[data_name] = {}\n",
    "    for thread_name in thread_names[data_name] + [\"All\"]:\n",
    "        n_threads += 1\n",
    "        if thread_name not in data_count[data_name]:\n",
    "            data_count[data_name][thread_name] = {}\n",
    "    for i in range(max_len[data_name]):\n",
    "        for thread_name in data[data_name][i]:\n",
    "            address = data[data_name][i][thread_name]\n",
    "            cache_level = cache_misses[data_name][thread_name][i]\n",
    "            maxx = max(maxx, address)\n",
    "            minx = min(minx, address)\n",
    "            l = cache_level\n",
    "            l = (l % 100)\n",
    "            m = (l % 10)\n",
    "            if m == 5:\n",
    "                hit = 6\n",
    "            elif m == 1:\n",
    "                hit = 1\n",
    "            elif m == 2:\n",
    "                hit = 2\n",
    "            elif l // 10 == 1:\n",
    "                hit = 3\n",
    "            elif m == 3:\n",
    "                hit = 4\n",
    "            elif m == 4:\n",
    "                hit = 5\n",
    "            else:\n",
    "                print(\"No hit: \", str(cache_level))\n",
    "                continue\n",
    "            data_key = str(hit) + \":\" + str(address) \n",
    "            if data_key in data_count[data_name][thread_name]:\n",
    "                data_count[data_name][thread_name][data_key] += 1\n",
    "            else:\n",
    "                data_count[data_name][thread_name][data_key] = 0\n",
    "        \n",
    "for data_name in data:\n",
    "    d = OrderedDict()\n",
    "    for thread_name in thread_names[data_name] + [\"All\"]:\n",
    "        d[thread_name] = {'address': [], 'count': [], 'cache_level': []}\n",
    "    if data_name not in data_count:\n",
    "        data_count[data_name] = {}\n",
    "    for thread_name in thread_names[data_name] + [\"All\"]:\n",
    "        n_threads += 1\n",
    "        if thread_name not in data_count[data_name]:\n",
    "            data_count[data_name][thread_name] = {}\n",
    "    for thread_name in data_count[data_name]:\n",
    "        for data_key in data_count[data_name][thread_name]:\n",
    "            hit, par, address = data_key.partition(\":\")\n",
    "            count = data_count[data_name][thread_name][data_key]\n",
    "            d[thread_name]['address'].append(int(address))\n",
    "            d[thread_name]['count'].append(count)\n",
    "            d[thread_name]['cache_level'].append(int(hit))\n",
    "            d[\"All\"]['address'].append(int(address))\n",
    "            d[\"All\"]['count'].append(count)  \n",
    "            d[\"All\"]['cache_level'].append(int(hit))\n",
    "            maxy = max(maxy, count)\n",
    "    data_frames[data_name] = OrderedDict()\n",
    "    for thread_name in thread_names[data_name] + [\"All\"]:\n",
    "        data_frames[data_name][thread_name] = pd.DataFrame(data=d[thread_name])\n",
    "        \n",
    "        \n",
    "end = timer()\n",
    "print(\"Time: \" + str(end - start) + \" s\")\n",
    "cache_levels = [1, 2, 3, 4, 5, 6]\n",
    "cache_level_keys = [\"L1 Hit\", \"L2 Hit\", \"L3 Hit Shared\", \"L3 Hit\", \"L3 Miss\", \"Modified\"]\n",
    "color_key = OrderedDict((k, c) for k, c in zip(cache_level_keys, Sets1to3[0:len(cache_level_keys)]))\n",
    "colors = hv.NdOverlay(OrderedDict((c, hv.Points([0,0], label=c).opts(style=dict(color=color_key[c]))) for c in cache_level_keys))\n",
    "scatter_dict = OrderedDict()\n",
    "opts = dict(x_range=(minx, maxx), y_range=(miny, maxy))\n",
    "threads = [str(th) for th in sorted(thread_names[data_name], key=lambda x: int(x))] + [\"All\"]\n",
    "for data_name in data:\n",
    "    for thread_name in threads:\n",
    "        df = data_frames[data_name][thread_name]\n",
    "        plot = OrderedDict((cache_level, hv.Scatter(df.loc[(df['cache_level'] == cache_level)], kdims=['address', 'count'])) for cache_level in cache_levels)\n",
    "        plot_data = hv.NdOverlay(plot, kdims='cache_level_keys')\n",
    "        scatter_dict[(data_name, thread_name)] = plot_data\n",
    "hmap = dynspread(datashade(hv.HoloMap(scatter_dict, kdims=['data_name', 'thread_name']), aggregator=ds.count_cat('cache_level_keys'), **opts), threshold=0.75, how='over') * colors\n",
    "end = timer()\n",
    "print(\"Time: \" + str(end - start) + \" s\")\n",
    "hmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
